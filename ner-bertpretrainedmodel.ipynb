{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-11-15T04:05:22.113660Z","iopub.execute_input":"2022-11-15T04:05:22.114433Z","iopub.status.idle":"2022-11-15T04:05:22.129269Z","shell.execute_reply.started":"2022-11-15T04:05:22.114371Z","shell.execute_reply":"2022-11-15T04:05:22.128081Z"},"trusted":true},"execution_count":191,"outputs":[{"name":"stdout","text":"/kaggle/input/twitter-dataset-for-namedentity-recognition/test.txt\n/kaggle/input/twitter-dataset-for-namedentity-recognition/train.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"#Most basic stuff for EDA.\n\nimport pandas as pd, numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Core packages for text processing.\n\nimport string \nimport re\n\n# Libraries for text preprocessing.\n\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('average_perceptron_tagger')\nnltk.download('wordnet')\nfrom nltk.corpus import stopwords, wordnet\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.probability import FreqDist\n\n# Loading some sklearn packaces for modelling.\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation, NMF\nfrom sklearn.metrics import f1_score, accuracy_score\n\n# Some packages for word clouds and NER.\n\nfrom wordcloud import WordCloud, STOPWORDS\nfrom collections import Counter, defaultdict\nfrom PIL import Image\nimport spacy\n!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz\nimport en_core_web_sm\n\n# Core packages for general use throughout the notebook.\n\nimport random\nimport warnings\nimport time\nimport datetime\n\n# For customizing our plots.\n\nfrom matplotlib.ticker import MaxNLocator\nimport matplotlib.gridspec as gridspec\nimport matplotlib.patches as mpatches\n\n# Loading pytorch packages.\n\nimport torch\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup\nfrom torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n\n# Setting some options for general use.\n\nstop = set(stopwords.words('english'))\nplt.style.use('fivethirtyeight')\nsns.set(font_scale=1.5)\npd.options.display.max_columns = 250\npd.options.display.max_rows = 250\nwarnings.filterwarnings('ignore')\n\n\n#Setting seeds for consistent results.\nseed_val = 42\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)","metadata":{"execution":{"iopub.status.busy":"2022-11-15T04:05:22.132058Z","iopub.execute_input":"2022-11-15T04:05:22.133037Z","iopub.status.idle":"2022-11-15T04:05:34.128752Z","shell.execute_reply.started":"2022-11-15T04:05:22.132995Z","shell.execute_reply":"2022-11-15T04:05:34.127483Z"},"trusted":true},"execution_count":192,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Error loading average_perceptron_tagger: Package\n[nltk_data]     'average_perceptron_tagger' not found in index\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nCollecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /opt/conda/lib/python3.7/site-packages (from en-core-web-sm==2.2.5) (3.3.1)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en-core-web-sm==2.2.5) (59.8.0)\nRequirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en-core-web-sm==2.2.5) (4.1.1)\nRequirement already satisfied: thinc<8.1.0,>=8.0.14 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en-core-web-sm==2.2.5) (8.0.17)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en-core-web-sm==2.2.5) (3.3.0)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en-core-web-sm==2.2.5) (2.28.1)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en-core-web-sm==2.2.5) (21.3)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en-core-web-sm==2.2.5) (3.0.8)\nRequirement already satisfied: pathy>=0.3.5 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en-core-web-sm==2.2.5) (0.6.2)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en-core-web-sm==2.2.5) (3.1.2)\nRequirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en-core-web-sm==2.2.5) (0.7.9)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en-core-web-sm==2.2.5) (4.64.0)\nRequirement already satisfied: wasabi<1.1.0,>=0.9.1 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en-core-web-sm==2.2.5) (0.10.1)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en-core-web-sm==2.2.5) (2.0.8)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en-core-web-sm==2.2.5) (2.0.7)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en-core-web-sm==2.2.5) (2.4.5)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en-core-web-sm==2.2.5) (1.8.2)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en-core-web-sm==2.2.5) (1.0.9)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en-core-web-sm==2.2.5) (1.0.3)\nRequirement already satisfied: typer<0.5.0,>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en-core-web-sm==2.2.5) (0.4.2)\nRequirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en-core-web-sm==2.2.5) (1.21.6)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->en-core-web-sm==2.2.5) (3.0.10)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.6->spacy>=2.2.2->en-core-web-sm==2.2.5) (3.8.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->spacy>=2.2.2->en-core-web-sm==2.2.5) (3.0.9)\nRequirement already satisfied: smart-open<6.0.0,>=5.2.1 in /opt/conda/lib/python3.7/site-packages (from pathy>=0.3.5->spacy>=2.2.2->en-core-web-sm==2.2.5) (5.2.1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en-core-web-sm==2.2.5) (1.26.12)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en-core-web-sm==2.2.5) (2022.9.24)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en-core-web-sm==2.2.5) (3.3)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en-core-web-sm==2.2.5) (2.1.0)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.7/site-packages (from typer<0.5.0,>=0.3.0->spacy>=2.2.2->en-core-web-sm==2.2.5) (8.0.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from jinja2->spacy>=2.2.2->en-core-web-sm==2.2.5) (2.1.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click<9.0.0,>=7.1.1->typer<0.5.0,>=0.3.0->spacy>=2.2.2->en-core-web-sm==2.2.5) (4.13.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# Loading the train and test data for visualization & exploration.\n\n#trainv = pd.read_csv('/kaggle/input/twitter-dataset-for-namedentity-recognition/train.txt',sep='\\t',names=['text','target'])\n#testv = pd.read_csv('/kaggle/input/twitter-dataset-for-namedentity-recognition/test.txt',sep='\\t')","metadata":{"execution":{"iopub.status.busy":"2022-11-15T04:05:34.131218Z","iopub.execute_input":"2022-11-15T04:05:34.131700Z","iopub.status.idle":"2022-11-15T04:05:34.140056Z","shell.execute_reply.started":"2022-11-15T04:05:34.131651Z","shell.execute_reply":"2022-11-15T04:05:34.138940Z"},"trusted":true},"execution_count":193,"outputs":[]},{"cell_type":"code","source":"file_path='/kaggle/input/twitter-dataset-for-namedentity-recognition/'\ndata_types = ['train', 'test']\ndataset_dict = dict()\nfor data_type in data_types:\n\n    with open(file_path + data_type + '.txt') as f:\n        xy_list = list()\n        tokens = list()\n        tags = list()\n        for line in f:\n            items = line.split()\n            if len(items) > 1 and '-DOCSTART-' not in items[0]:\n                token, tag = items\n            \n                if token.find('http://') == 0 or token.find('https://') == 0:\n                    token = '<URL>'                \n\n                if token[0].isdigit():\n                    tokens.append('#')\n                elif token[0] == '@':\n                    token = '<USR>'\n                else:\n                    tokens.append(token)\n                tags.append(tag)\n            elif len(tokens) > 0:\n                xy_list.append((tokens, tags,))\n                tokens = list()\n                tags = list()\n        dataset_dict[data_type] = xy_list\n\nfor key in dataset_dict:\n    print('Number of samples (sentences) in {:<5}: {}'.format(key, len(dataset_dict[key])))\n\nprint('\\nHere is a first two samples from the train part of the dataset:')\nfirst_two_train_samples = dataset_dict['train'][:2]\nfor n, sample in enumerate(first_two_train_samples):\n    # sample is a tuple of sentence_tokens and sentence_tags\n    tokens, tags = sample\n    print('Sentence {}'.format(n))\n    print('Tokens: {}'.format(tokens))\n    print('Tags:   {}'.format(tags))","metadata":{"execution":{"iopub.status.busy":"2022-11-15T04:05:34.143633Z","iopub.execute_input":"2022-11-15T04:05:34.143993Z","iopub.status.idle":"2022-11-15T04:05:34.270174Z","shell.execute_reply.started":"2022-11-15T04:05:34.143965Z","shell.execute_reply":"2022-11-15T04:05:34.269106Z"},"trusted":true},"execution_count":194,"outputs":[{"name":"stdout","text":"Number of samples (sentences) in train: 3394\nNumber of samples (sentences) in test : 0\n\nHere is a first two samples from the train part of the dataset:\nSentence 0\nTokens: ['It', \"'s\", 'the', 'view', 'from', 'where', 'I', \"'m\", 'living', 'for', 'two', 'weeks', '.', 'Empire', 'State', 'Building', '=', 'ESB', '.', 'Pretty', 'bad', 'storm', 'here', 'last', 'evening', '.']\nTags:   ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-location', 'I-location', 'I-location', 'O', 'B-location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\nSentence 1\nTokens: ['From', 'Green', 'Newsfeed', ':', 'AHFA', 'extends', 'deadline', 'for', 'Sage', 'Award', 'to', 'Nov', '.', '#', '<URL>']\nTags:   ['O', 'O', 'O', 'O', 'B-group', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n","output_type":"stream"}]},{"cell_type":"code","source":"type(dataset_dict)","metadata":{"execution":{"iopub.status.busy":"2022-11-15T04:05:34.271680Z","iopub.execute_input":"2022-11-15T04:05:34.272109Z","iopub.status.idle":"2022-11-15T04:05:34.279825Z","shell.execute_reply.started":"2022-11-15T04:05:34.272064Z","shell.execute_reply":"2022-11-15T04:05:34.278575Z"},"trusted":true},"execution_count":195,"outputs":[{"execution_count":195,"output_type":"execute_result","data":{"text/plain":"dict"},"metadata":{}}]},{"cell_type":"code","source":"dataset_dict['test'][:2]","metadata":{"execution":{"iopub.status.busy":"2022-11-15T04:47:28.381407Z","iopub.execute_input":"2022-11-15T04:47:28.382147Z","iopub.status.idle":"2022-11-15T04:47:28.388721Z","shell.execute_reply.started":"2022-11-15T04:47:28.382109Z","shell.execute_reply":"2022-11-15T04:47:28.387565Z"},"trusted":true},"execution_count":238,"outputs":[{"execution_count":238,"output_type":"execute_result","data":{"text/plain":"[]"},"metadata":{}}]},{"cell_type":"code","source":"df=pd.DataFrame(dataset_dict['train'],columns=['text','labels'])","metadata":{"execution":{"iopub.status.busy":"2022-11-15T04:05:34.281479Z","iopub.execute_input":"2022-11-15T04:05:34.282794Z","iopub.status.idle":"2022-11-15T04:05:34.292733Z","shell.execute_reply.started":"2022-11-15T04:05:34.282553Z","shell.execute_reply":"2022-11-15T04:05:34.291761Z"},"trusted":true},"execution_count":196,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-15T04:05:34.294154Z","iopub.execute_input":"2022-11-15T04:05:34.294623Z","iopub.status.idle":"2022-11-15T04:05:34.315285Z","shell.execute_reply.started":"2022-11-15T04:05:34.294586Z","shell.execute_reply":"2022-11-15T04:05:34.313482Z"},"trusted":true},"execution_count":197,"outputs":[{"execution_count":197,"output_type":"execute_result","data":{"text/plain":"                                                text  \\\n0  [It, 's, the, view, from, where, I, 'm, living...   \n1  [From, Green, Newsfeed, :, AHFA, extends, dead...   \n2  [Pxleyes, Top, #, Photography, Contest, Pictur...   \n3     [today, is, my, last, day, at, the, office, .]   \n4  [#, 's, place, til, monday, ,, party, party, p...   \n\n                                              labels  \n0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-l...  \n1  [O, O, O, O, B-group, O, O, O, O, O, O, O, O, ...  \n2   [B-corporation, O, O, O, O, O, O, O, O, O, O, O]  \n3                        [O, O, O, O, O, O, O, O, O]  \n4        [B-person, O, O, O, O, O, O, O, O, O, O, O]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[It, 's, the, view, from, where, I, 'm, living...</td>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-l...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[From, Green, Newsfeed, :, AHFA, extends, dead...</td>\n      <td>[O, O, O, O, B-group, O, O, O, O, O, O, O, O, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[Pxleyes, Top, #, Photography, Contest, Pictur...</td>\n      <td>[B-corporation, O, O, O, O, O, O, O, O, O, O, O]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[today, is, my, last, day, at, the, office, .]</td>\n      <td>[O, O, O, O, O, O, O, O, O]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[#, 's, place, til, monday, ,, party, party, p...</td>\n      <td>[B-person, O, O, O, O, O, O, O, O, O, O, O]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Taking general look at the both datasets.\n\n#display(trainv.sample(5))\n#display(testv.sample(5))","metadata":{"execution":{"iopub.status.busy":"2022-11-15T04:05:34.316958Z","iopub.execute_input":"2022-11-15T04:05:34.317257Z","iopub.status.idle":"2022-11-15T04:05:34.325575Z","shell.execute_reply.started":"2022-11-15T04:05:34.317230Z","shell.execute_reply":"2022-11-15T04:05:34.324529Z"},"trusted":true},"execution_count":198,"outputs":[]},{"cell_type":"code","source":"# Some basic helper functions to clean text by removing urls, emojis, html tags and punctuations.\n\ndef remove_URL(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'', text)\n\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\n        '['\n        u'\\U0001F600-\\U0001F64F'  # emoticons\n        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n        u'\\U00002702-\\U000027B0'\n        u'\\U000024C2-\\U0001F251'\n        ']+',\n        flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n\ndef remove_html(text):\n    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n    return re.sub(html, '', text)\n\n\ndef remove_punct(text):\n    table = str.maketrans('', '', string.punctuation)\n    return text.translate(table)\n","metadata":{"execution":{"iopub.status.busy":"2022-11-15T04:05:34.329928Z","iopub.execute_input":"2022-11-15T04:05:34.330236Z","iopub.status.idle":"2022-11-15T04:05:34.340272Z","shell.execute_reply.started":"2022-11-15T04:05:34.330195Z","shell.execute_reply":"2022-11-15T04:05:34.339199Z"},"trusted":true},"execution_count":199,"outputs":[]},{"cell_type":"code","source":"# Applying helper functions\n\n#trainv['text_clean'] = trainv['text'].apply(lambda x: remove_URL(x))\n#trainv['text_clean'] = trainv['text_clean'].apply(lambda x: remove_emoji(x))\n#trainv['text_clean'] = trainv['text_clean'].apply(lambda x: remove_html(x))\n#trainv['text_clean'] = trainv['text_clean'].apply(lambda x: remove_punct(x))","metadata":{"execution":{"iopub.status.busy":"2022-11-15T04:05:34.341688Z","iopub.execute_input":"2022-11-15T04:05:34.342539Z","iopub.status.idle":"2022-11-15T04:05:34.359516Z","shell.execute_reply.started":"2022-11-15T04:05:34.342501Z","shell.execute_reply":"2022-11-15T04:05:34.358368Z"},"trusted":true},"execution_count":200,"outputs":[]},{"cell_type":"code","source":"# Split labels based on whitespace and turn them into a list\nlabels = [i for i in df['labels'].values.tolist()]\n\n# Check how many labels are there in the dataset\nunique_labels = set()\n\nfor lb in labels:\n  [unique_labels.add(i) for i in lb if i not in unique_labels]\n \nprint(unique_labels)\n#{'B-tim', 'B-art', 'I-art', 'O', 'I-gpe', 'I-per', 'I-nat', 'I-geo', 'B-eve', 'B-org', 'B-gpe', 'I-eve', 'B-per', 'I-tim', 'B-nat', 'B-geo', 'I-org'}\n\n# Map each label into its id representation and vice versa\nlabels_to_ids = {k: v for v, k in enumerate(sorted(unique_labels))}\nids_to_labels = {v: k for v, k in enumerate(sorted(unique_labels))}\nprint(labels_to_ids)\n#{'B-art': 0, 'B-eve': 1, 'B-geo': 2, 'B-gpe': 3, 'B-nat': 4, 'B-org': 5, 'B-per': 6, 'B-tim': 7, 'I-art': 8, 'I-eve': 9, 'I-geo': 10, 'I-gpe': 11, 'I-nat': 12, 'I-org': 13, 'I-per': 14, 'I-tim': 15, 'O': 16}\n","metadata":{"execution":{"iopub.status.busy":"2022-11-15T04:05:34.361032Z","iopub.execute_input":"2022-11-15T04:05:34.361994Z","iopub.status.idle":"2022-11-15T04:05:34.380374Z","shell.execute_reply.started":"2022-11-15T04:05:34.361955Z","shell.execute_reply":"2022-11-15T04:05:34.379384Z"},"trusted":true},"execution_count":201,"outputs":[{"name":"stdout","text":"{'B-product', 'B-corporation', 'I-person', 'I-location', 'O', 'B-person', 'I-group', 'I-creative-work', 'I-corporation', 'B-group', 'B-creative-work', 'I-product', 'B-location'}\n{'B-corporation': 0, 'B-creative-work': 1, 'B-group': 2, 'B-location': 3, 'B-person': 4, 'B-product': 5, 'I-corporation': 6, 'I-creative-work': 7, 'I-group': 8, 'I-location': 9, 'I-person': 10, 'I-product': 11, 'O': 12}\n","output_type":"stream"}]},{"cell_type":"code","source":"# Let's take a look at how can we preprocess the text - Take first example\n#text = df['text'].values.tolist()\ntext = df['text'].str.join(\" \")\n#text = df['text']\nexample = text[0]\n\nprint(example)\n#>>> 'Prime Minister Geir Haarde has refused to resign or call for early elections.'","metadata":{"execution":{"iopub.status.busy":"2022-11-15T04:05:34.382197Z","iopub.execute_input":"2022-11-15T04:05:34.383332Z","iopub.status.idle":"2022-11-15T04:05:34.401307Z","shell.execute_reply.started":"2022-11-15T04:05:34.383292Z","shell.execute_reply":"2022-11-15T04:05:34.400127Z"},"trusted":true},"execution_count":202,"outputs":[{"name":"stdout","text":"It 's the view from where I 'm living for two weeks . Empire State Building = ESB . Pretty bad storm here last evening .\n","output_type":"stream"}]},{"cell_type":"code","source":"#Find length of longest string in Pandas dataframe column\n#%timeit -n 100 df.col1.str.len().max()\n#100 loops, best of 3: 11.7 ms per loop\n\n#%timeit -n 100 df.col1.map(lambda x: len(x)).max()\n#100 loops, best of 3: 16.4 ms per loop\n\n#%timeit -n 100 df.col1.map(len).max()\n#100 loops, best of 3: 10.1 ms per loop\ndf.text.map(len).max()","metadata":{"execution":{"iopub.status.busy":"2022-11-15T04:05:34.402994Z","iopub.execute_input":"2022-11-15T04:05:34.403560Z","iopub.status.idle":"2022-11-15T04:05:34.419111Z","shell.execute_reply.started":"2022-11-15T04:05:34.403524Z","shell.execute_reply":"2022-11-15T04:05:34.417969Z"},"trusted":true},"execution_count":203,"outputs":[{"execution_count":203,"output_type":"execute_result","data":{"text/plain":"41"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import BertTokenizerFast\n\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n","metadata":{"execution":{"iopub.status.busy":"2022-11-15T04:05:34.420956Z","iopub.execute_input":"2022-11-15T04:05:34.421247Z","iopub.status.idle":"2022-11-15T04:05:35.638969Z","shell.execute_reply.started":"2022-11-15T04:05:34.421219Z","shell.execute_reply":"2022-11-15T04:05:35.637812Z"},"trusted":true},"execution_count":204,"outputs":[]},{"cell_type":"code","source":"text_tokenized = tokenizer(example, padding='max_length', max_length=50, truncation=True, return_tensors=\"pt\")","metadata":{"execution":{"iopub.status.busy":"2022-11-15T04:05:35.641823Z","iopub.execute_input":"2022-11-15T04:05:35.642595Z","iopub.status.idle":"2022-11-15T04:05:35.649279Z","shell.execute_reply.started":"2022-11-15T04:05:35.642553Z","shell.execute_reply":"2022-11-15T04:05:35.648050Z"},"trusted":true},"execution_count":205,"outputs":[]},{"cell_type":"code","source":"print(text_tokenized)","metadata":{"execution":{"iopub.status.busy":"2022-11-15T04:05:35.651196Z","iopub.execute_input":"2022-11-15T04:05:35.651872Z","iopub.status.idle":"2022-11-15T04:05:35.662204Z","shell.execute_reply.started":"2022-11-15T04:05:35.651828Z","shell.execute_reply":"2022-11-15T04:05:35.660924Z"},"trusted":true},"execution_count":206,"outputs":[{"name":"stdout","text":"{'input_ids': tensor([[  101,  1135,   112,   188,  1103,  2458,  1121,  1187,   146,   112,\n           182,  1690,  1111,  1160,  2277,   119,  2813,  1426,  4334,   134,\n           142, 19117,   119, 12004,  2213,  4162,  1303,  1314,  3440,   119,\n           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0]])}\n","output_type":"stream"}]},{"cell_type":"code","source":"print(tokenizer.decode(text_tokenized.input_ids[0]))","metadata":{"execution":{"iopub.status.busy":"2022-11-15T04:05:35.664259Z","iopub.execute_input":"2022-11-15T04:05:35.664736Z","iopub.status.idle":"2022-11-15T04:05:35.674059Z","shell.execute_reply.started":"2022-11-15T04:05:35.664698Z","shell.execute_reply":"2022-11-15T04:05:35.672631Z"},"trusted":true},"execution_count":207,"outputs":[{"name":"stdout","text":"[CLS] It's the view from where I'm living for two weeks. Empire State Building = ESB. Pretty bad storm here last evening. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(tokenizer.convert_ids_to_tokens(text_tokenized[\"input_ids\"][0]))\n","metadata":{"execution":{"iopub.status.busy":"2022-11-15T04:05:35.676082Z","iopub.execute_input":"2022-11-15T04:05:35.676550Z","iopub.status.idle":"2022-11-15T04:05:35.687029Z","shell.execute_reply.started":"2022-11-15T04:05:35.676513Z","shell.execute_reply":"2022-11-15T04:05:35.685951Z"},"trusted":true},"execution_count":208,"outputs":[{"name":"stdout","text":"['[CLS]', 'It', \"'\", 's', 'the', 'view', 'from', 'where', 'I', \"'\", 'm', 'living', 'for', 'two', 'weeks', '.', 'Empire', 'State', 'Building', '=', 'E', '##SB', '.', 'Pretty', 'bad', 'storm', 'here', 'last', 'evening', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n","output_type":"stream"}]},{"cell_type":"code","source":"word_ids = text_tokenized.word_ids()\nprint(tokenizer.convert_ids_to_tokens(text_tokenized[\"input_ids\"][0]))\nprint(word_ids)\n","metadata":{"execution":{"iopub.status.busy":"2022-11-15T04:05:35.688079Z","iopub.execute_input":"2022-11-15T04:05:35.688433Z","iopub.status.idle":"2022-11-15T04:05:35.700381Z","shell.execute_reply.started":"2022-11-15T04:05:35.688387Z","shell.execute_reply":"2022-11-15T04:05:35.699088Z"},"trusted":true},"execution_count":209,"outputs":[{"name":"stdout","text":"['[CLS]', 'It', \"'\", 's', 'the', 'view', 'from', 'where', 'I', \"'\", 'm', 'living', 'for', 'two', 'weeks', '.', 'Empire', 'State', 'Building', '=', 'E', '##SB', '.', 'Pretty', 'bad', 'storm', 'here', 'last', 'evening', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 19, 20, 21, 22, 23, 24, 25, 26, 27, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n","output_type":"stream"}]},{"cell_type":"code","source":"def align_label_example(tokenized_input, labels):\n\n        word_ids = tokenized_input.word_ids()\n\n        previous_word_idx = None\n        label_ids = []\n   \n        for word_idx in word_ids:\n\n            if word_idx is None:\n                label_ids.append(-100)\n                \n            elif word_idx != previous_word_idx:\n                try:\n                  label_ids.append(labels_to_ids[labels[word_idx]])\n                except:\n                  label_ids.append(-100)\n        \n            else:\n                label_ids.append(labels_to_ids[labels[word_idx]] if label_all_tokens else -100)\n            previous_word_idx = word_idx\n      \n\n        return label_ids","metadata":{"execution":{"iopub.status.busy":"2022-11-15T04:05:35.701868Z","iopub.execute_input":"2022-11-15T04:05:35.702867Z","iopub.status.idle":"2022-11-15T04:05:35.711786Z","shell.execute_reply.started":"2022-11-15T04:05:35.702830Z","shell.execute_reply":"2022-11-15T04:05:35.710624Z"},"trusted":true},"execution_count":210,"outputs":[]},{"cell_type":"code","source":"label = labels[0]\n\n#If we set label_all_tokens to True.....\nlabel_all_tokens = True\n\nnew_label = align_label_example(text_tokenized, label)\nprint(new_label)\nprint(tokenizer.convert_ids_to_tokens(text_tokenized[\"input_ids\"][0]))\n\n#>>> [-100, 16, 16, 6, 6, 6, 14, 14, 14, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n#>>> ['[CLS]', 'Prime', 'Minister', 'G', '##ei', '##r', 'Ha', '##ard', '##e', 'has', 'refused', 'to', 'resign', 'or', 'call', 'for', 'early', 'elections', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']","metadata":{"execution":{"iopub.status.busy":"2022-11-15T04:05:35.714997Z","iopub.execute_input":"2022-11-15T04:05:35.715389Z","iopub.status.idle":"2022-11-15T04:05:35.735599Z","shell.execute_reply.started":"2022-11-15T04:05:35.715361Z","shell.execute_reply":"2022-11-15T04:05:35.734464Z"},"trusted":true},"execution_count":211,"outputs":[{"name":"stdout","text":"[-100, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 3, 9, 9, 12, 3, 12, 12, 12, 12, 12, 12, 12, 12, 12, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n['[CLS]', 'It', \"'\", 's', 'the', 'view', 'from', 'where', 'I', \"'\", 'm', 'living', 'for', 'two', 'weeks', '.', 'Empire', 'State', 'Building', '=', 'E', '##SB', '.', 'Pretty', 'bad', 'storm', 'here', 'last', 'evening', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n","output_type":"stream"}]},{"cell_type":"code","source":"#If we set label_all_tokens to False.....\nlabel_all_tokens = False\n\nnew_label = align_label_example(text_tokenized, label)\nprint(new_label)\nprint(tokenizer.convert_ids_to_tokens(text_tokenized[\"input_ids\"][0]))\n\n#>>> [-100, 16, 16, 6, -100, -100, 14, -100, -100, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n#>>> ['[CLS]', 'Prime', 'Minister', 'G', '##ei', '##r', 'Ha', '##ard', '##e', 'has', 'refused', 'to', 'resign', 'or', 'call', 'for', 'early', 'elections', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n","metadata":{"execution":{"iopub.status.busy":"2022-11-15T04:05:35.737439Z","iopub.execute_input":"2022-11-15T04:05:35.738135Z","iopub.status.idle":"2022-11-15T04:05:35.756670Z","shell.execute_reply.started":"2022-11-15T04:05:35.738098Z","shell.execute_reply":"2022-11-15T04:05:35.755465Z"},"trusted":true},"execution_count":212,"outputs":[{"name":"stdout","text":"[-100, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 3, 9, 9, 12, 3, 12, -100, 12, 12, 12, 12, 12, 12, 12, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n['[CLS]', 'It', \"'\", 's', 'the', 'view', 'from', 'where', 'I', \"'\", 'm', 'living', 'for', 'two', 'weeks', '.', 'Empire', 'State', 'Building', '=', 'E', '##SB', '.', 'Pretty', 'bad', 'storm', 'here', 'last', 'evening', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Dataset Class\nBefore we train our BERT model for NER task, we need to create a dataset class to generate and fetch data in a batch.","metadata":{}},{"cell_type":"code","source":"import torch\n\ndef align_label(texts, labels):\n    tokenized_inputs = tokenizer(texts, padding='max_length', max_length=50, truncation=True)\n\n    word_ids = tokenized_inputs.word_ids()\n\n    previous_word_idx = None\n    label_ids = []\n\n    for word_idx in word_ids:\n\n        if word_idx is None:\n            label_ids.append(-100)\n\n        elif word_idx != previous_word_idx:\n            try:\n                label_ids.append(labels_to_ids[labels[word_idx]])\n            except:\n                label_ids.append(-100)\n        else:\n            try:\n                label_ids.append(labels_to_ids[labels[word_idx]] if label_all_tokens else -100)\n            except:\n                label_ids.append(-100)\n        previous_word_idx = word_idx\n\n    return label_ids\n\nclass DataSequence(torch.utils.data.Dataset):\n\n    def __init__(self, df):\n\n        #lb = [i.split() for i in df['labels'].values.tolist()]\n        lb = [i for i in df['labels'].values.tolist()]\n        txt = df['text'].values.tolist()\n        self.texts = [tokenizer(str(i),\n                               padding='max_length', max_length = 50, truncation=True, return_tensors=\"pt\") for i in txt]\n        self.labels = [align_label(i,j) for i,j in zip(txt, lb)]\n\n    def __len__(self):\n\n        return len(self.labels)\n\n    def get_batch_data(self, idx):\n\n        return self.texts[idx]\n\n    def get_batch_labels(self, idx):\n\n        return torch.LongTensor(self.labels[idx])\n\n    def __getitem__(self, idx):\n\n        batch_data = self.get_batch_data(idx)\n        batch_labels = self.get_batch_labels(idx)\n\n        return batch_data, batch_labels","metadata":{"execution":{"iopub.status.busy":"2022-11-15T04:05:35.758353Z","iopub.execute_input":"2022-11-15T04:05:35.759574Z","iopub.status.idle":"2022-11-15T04:05:35.775438Z","shell.execute_reply.started":"2022-11-15T04:05:35.759535Z","shell.execute_reply":"2022-11-15T04:05:35.774286Z"},"trusted":true},"execution_count":213,"outputs":[]},{"cell_type":"code","source":"df['text'] = df['text'].str.join(\" \")","metadata":{"execution":{"iopub.status.busy":"2022-11-15T04:05:35.783317Z","iopub.execute_input":"2022-11-15T04:05:35.783713Z","iopub.status.idle":"2022-11-15T04:05:35.795254Z","shell.execute_reply.started":"2022-11-15T04:05:35.783665Z","shell.execute_reply":"2022-11-15T04:05:35.794098Z"},"trusted":true},"execution_count":214,"outputs":[]},{"cell_type":"code","source":"df.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-11-15T04:05:35.796798Z","iopub.execute_input":"2022-11-15T04:05:35.798575Z","iopub.status.idle":"2022-11-15T04:05:35.816592Z","shell.execute_reply.started":"2022-11-15T04:05:35.798525Z","shell.execute_reply":"2022-11-15T04:05:35.815371Z"},"trusted":true},"execution_count":215,"outputs":[{"execution_count":215,"output_type":"execute_result","data":{"text/plain":"                                                text  \\\n0  It 's the view from where I 'm living for two ...   \n1  From Green Newsfeed : AHFA extends deadline fo...   \n2  Pxleyes Top # Photography Contest Pictures of ...   \n3               today is my last day at the office .   \n4  # 's place til monday , party party party . &l...   \n5  watching the VMA pre-show again lol it was n't...   \n6   # followers ! # followers is my goal for today !   \n7  This is the # hospital ive been in today , but...   \n8                            Friday Night Eats <URL>   \n9  Gotta dress up for london fashion week and par...   \n\n                                              labels  \n0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-l...  \n1  [O, O, O, O, B-group, O, O, O, O, O, O, O, O, ...  \n2   [B-corporation, O, O, O, O, O, O, O, O, O, O, O]  \n3                        [O, O, O, O, O, O, O, O, O]  \n4        [B-person, O, O, O, O, O, O, O, O, O, O, O]  \n5  [O, O, B-creative-work, O, O, O, O, O, O, O, O...  \n6                  [O, O, O, O, O, O, O, O, O, O, O]  \n7  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n8                                       [O, O, O, O]  \n9               [O, O, O, O, O, O, O, O, O, O, O, O]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>It 's the view from where I 'm living for two ...</td>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-l...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>From Green Newsfeed : AHFA extends deadline fo...</td>\n      <td>[O, O, O, O, B-group, O, O, O, O, O, O, O, O, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Pxleyes Top # Photography Contest Pictures of ...</td>\n      <td>[B-corporation, O, O, O, O, O, O, O, O, O, O, O]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>today is my last day at the office .</td>\n      <td>[O, O, O, O, O, O, O, O, O]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td># 's place til monday , party party party . &amp;l...</td>\n      <td>[B-person, O, O, O, O, O, O, O, O, O, O, O]</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>watching the VMA pre-show again lol it was n't...</td>\n      <td>[O, O, B-creative-work, O, O, O, O, O, O, O, O...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td># followers ! # followers is my goal for today !</td>\n      <td>[O, O, O, O, O, O, O, O, O, O, O]</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>This is the # hospital ive been in today , but...</td>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Friday Night Eats &lt;URL&gt;</td>\n      <td>[O, O, O, O]</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Gotta dress up for london fashion week and par...</td>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#df1=df","metadata":{"execution":{"iopub.status.busy":"2022-11-15T04:05:35.818494Z","iopub.execute_input":"2022-11-15T04:05:35.819609Z","iopub.status.idle":"2022-11-15T04:05:35.826590Z","shell.execute_reply.started":"2022-11-15T04:05:35.819568Z","shell.execute_reply":"2022-11-15T04:05:35.825460Z"},"trusted":true},"execution_count":216,"outputs":[]},{"cell_type":"code","source":"#df1.labels = list(map(lambda x: x.replace('B-corporation','B-org'), df1.labels))\n\n#df1.labels.replace('B-corporation','B-org')","metadata":{"execution":{"iopub.status.busy":"2022-11-15T04:05:35.828097Z","iopub.execute_input":"2022-11-15T04:05:35.829025Z","iopub.status.idle":"2022-11-15T04:05:35.842368Z","shell.execute_reply.started":"2022-11-15T04:05:35.828983Z","shell.execute_reply":"2022-11-15T04:05:35.841289Z"},"trusted":true},"execution_count":217,"outputs":[]},{"cell_type":"code","source":"df1.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-15T04:05:35.843944Z","iopub.execute_input":"2022-11-15T04:05:35.844508Z","iopub.status.idle":"2022-11-15T04:05:35.861064Z","shell.execute_reply.started":"2022-11-15T04:05:35.844453Z","shell.execute_reply":"2022-11-15T04:05:35.860095Z"},"trusted":true},"execution_count":218,"outputs":[{"execution_count":218,"output_type":"execute_result","data":{"text/plain":"                                                text  \\\n0  It 's the view from where I 'm living for two ...   \n1  From Green Newsfeed : AHFA extends deadline fo...   \n2  Pxleyes Top # Photography Contest Pictures of ...   \n3               today is my last day at the office .   \n4  # 's place til monday , party party party . &l...   \n\n                                              labels  \n0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-l...  \n1  [O, O, O, O, B-group, O, O, O, O, O, O, O, O, ...  \n2   [B-corporation, O, O, O, O, O, O, O, O, O, O, O]  \n3                        [O, O, O, O, O, O, O, O, O]  \n4        [B-person, O, O, O, O, O, O, O, O, O, O, O]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>It 's the view from where I 'm living for two ...</td>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-l...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>From Green Newsfeed : AHFA extends deadline fo...</td>\n      <td>[O, O, O, O, B-group, O, O, O, O, O, O, O, O, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Pxleyes Top # Photography Contest Pictures of ...</td>\n      <td>[B-corporation, O, O, O, O, O, O, O, O, O, O, O]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>today is my last day at the office .</td>\n      <td>[O, O, O, O, O, O, O, O, O]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td># 's place til monday , party party party . &amp;l...</td>\n      <td>[B-person, O, O, O, O, O, O, O, O, O, O, O]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2022-11-15T04:05:35.863673Z","iopub.execute_input":"2022-11-15T04:05:35.864533Z","iopub.status.idle":"2022-11-15T04:05:35.872998Z","shell.execute_reply.started":"2022-11-15T04:05:35.864496Z","shell.execute_reply":"2022-11-15T04:05:35.871815Z"},"trusted":true},"execution_count":219,"outputs":[{"execution_count":219,"output_type":"execute_result","data":{"text/plain":"(3394, 2)"},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\n\ndf = df[0:3000]\ndf_train, df_val, df_test = np.split(df.sample(frac=1, random_state=42),[int(.8 * len(df)), int(.9 * len(df))])\n\n#df_train, df_val = np.split(df.sample(frac=1, random_state=42),[int(.8 * len(df))])","metadata":{"execution":{"iopub.status.busy":"2022-11-15T04:05:35.874978Z","iopub.execute_input":"2022-11-15T04:05:35.875346Z","iopub.status.idle":"2022-11-15T04:05:35.885673Z","shell.execute_reply.started":"2022-11-15T04:05:35.875287Z","shell.execute_reply":"2022-11-15T04:05:35.884664Z"},"trusted":true},"execution_count":220,"outputs":[]},{"cell_type":"code","source":"df_train.shape, df_val.shape","metadata":{"execution":{"iopub.status.busy":"2022-11-15T04:05:35.887385Z","iopub.execute_input":"2022-11-15T04:05:35.887788Z","iopub.status.idle":"2022-11-15T04:05:35.903917Z","shell.execute_reply.started":"2022-11-15T04:05:35.887753Z","shell.execute_reply":"2022-11-15T04:05:35.902876Z"},"trusted":true},"execution_count":221,"outputs":[{"execution_count":221,"output_type":"execute_result","data":{"text/plain":"((2400, 2), (300, 2))"},"metadata":{}}]},{"cell_type":"markdown","source":"### Model Building\nIn this article, we’re going to use a pretrained BERT base model from HuggingFace. Since we’re going to classify text in the token level, then we need to use BertForTokenClassification class.\n\nBertForTokenClassification class is a model that wraps BERT model and adds linear layers on top of BERT model that will act as token-level classifiers.","metadata":{}},{"cell_type":"code","source":"from transformers import BertForTokenClassification\n\nclass BertModel(torch.nn.Module):\n\n    def __init__(self):\n\n        super(BertModel, self).__init__()\n\n        self.bert = BertForTokenClassification.from_pretrained('bert-base-cased', num_labels=len(unique_labels))\n\n    def forward(self, input_id, mask, label):\n\n        output = self.bert(input_ids=input_id, attention_mask=mask, labels=label, return_dict=False)\n\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-11-15T04:05:35.905094Z","iopub.execute_input":"2022-11-15T04:05:35.905390Z","iopub.status.idle":"2022-11-15T04:05:35.915540Z","shell.execute_reply.started":"2022-11-15T04:05:35.905356Z","shell.execute_reply":"2022-11-15T04:05:35.914697Z"},"trusted":true},"execution_count":222,"outputs":[]},{"cell_type":"markdown","source":"### Training Loop\n\nThe training loop for our BERT model is the standard PyTorch training loop with a few additions, as you can see below:","metadata":{}},{"cell_type":"code","source":"from torch.optim import SGD\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2022-11-15T04:05:35.918783Z","iopub.execute_input":"2022-11-15T04:05:35.919076Z","iopub.status.idle":"2022-11-15T04:05:35.927649Z","shell.execute_reply.started":"2022-11-15T04:05:35.919049Z","shell.execute_reply":"2022-11-15T04:05:35.926810Z"},"trusted":true},"execution_count":223,"outputs":[]},{"cell_type":"code","source":"def train_loop(model, df_train, df_val):\n\n    train_dataset = DataSequence(df_train)\n    val_dataset = DataSequence(df_val)\n\n    train_dataloader = DataLoader(train_dataset, num_workers=4, batch_size=BATCH_SIZE, shuffle=True)\n    val_dataloader = DataLoader(val_dataset, num_workers=4, batch_size=BATCH_SIZE)\n\n    use_cuda = torch.cuda.is_available()\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n    optimizer = SGD(model.parameters(), lr=LEARNING_RATE)\n\n    if use_cuda:\n        model = model.cuda()\n\n    best_acc = 0\n    best_loss = 1000\n\n    for epoch_num in range(EPOCHS):\n\n        total_acc_train = 0\n        total_loss_train = 0\n\n        model.train()\n\n        for train_data, train_label in tqdm(train_dataloader):\n\n            train_label = train_label.to(device)\n            mask = train_data['attention_mask'].squeeze(1).to(device)\n            input_id = train_data['input_ids'].squeeze(1).to(device)\n\n            optimizer.zero_grad()\n            loss, logits = model(input_id, mask, train_label)\n\n            for i in range(logits.shape[0]):\n\n              logits_clean = logits[i][train_label[i] != -100]\n              label_clean = train_label[i][train_label[i] != -100]\n              #In each epoch of the training loop, there is also an important step that we need to do. \n              #After model prediction, we need to ignore all of the tokens that have ‘-100’ as the label  \n\n              predictions = logits_clean.argmax(dim=1)\n              acc = (predictions == label_clean).float().mean()\n              total_acc_train += acc\n              total_loss_train += loss.item()\n\n            loss.backward()\n            optimizer.step()\n\n        model.eval()\n\n        total_acc_val = 0\n        total_loss_val = 0\n\n        for val_data, val_label in val_dataloader:\n\n            val_label = val_label.to(device)\n            mask = val_data['attention_mask'].squeeze(1).to(device)\n            input_id = val_data['input_ids'].squeeze(1).to(device)\n\n            loss, logits = model(input_id, mask, val_label)\n\n            for i in range(logits.shape[0]):\n\n              logits_clean = logits[i][val_label[i] != -100]\n              label_clean = val_label[i][val_label[i] != -100]\n\n              predictions = logits_clean.argmax(dim=1)\n              acc = (predictions == label_clean).float().mean()\n              total_acc_val += acc\n              total_loss_val += loss.item()\n\n        val_accuracy = total_acc_val / len(df_val)\n        val_loss = total_loss_val / len(df_val)\n\n        print(\n            f'Epochs: {epoch_num + 1} | Loss: {total_loss_train / len(df_train): .3f} | Accuracy: {total_acc_train / len(df_train): .3f} | Val_Loss: {total_loss_val / len(df_val): .3f} | Accuracy: {total_acc_val / len(df_val): .3f}')\n\n","metadata":{"execution":{"iopub.status.busy":"2022-11-15T04:05:35.929703Z","iopub.execute_input":"2022-11-15T04:05:35.930028Z","iopub.status.idle":"2022-11-15T04:05:35.946940Z","shell.execute_reply.started":"2022-11-15T04:05:35.929994Z","shell.execute_reply":"2022-11-15T04:05:35.945899Z"},"trusted":true},"execution_count":224,"outputs":[]},{"cell_type":"code","source":"LEARNING_RATE = 5e-3\nEPOCHS = 5\nBATCH_SIZE = 2\n\nmodel = BertModel()","metadata":{"execution":{"iopub.status.busy":"2022-11-15T04:05:35.948599Z","iopub.execute_input":"2022-11-15T04:05:35.949480Z","iopub.status.idle":"2022-11-15T04:05:38.064326Z","shell.execute_reply.started":"2022-11-15T04:05:35.949393Z","shell.execute_reply":"2022-11-15T04:05:38.063280Z"},"trusted":true},"execution_count":225,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"train_loop(model, df_train, df_val)","metadata":{"execution":{"iopub.status.busy":"2022-11-15T04:05:38.066532Z","iopub.execute_input":"2022-11-15T04:05:38.067232Z","iopub.status.idle":"2022-11-15T04:10:40.130853Z","shell.execute_reply.started":"2022-11-15T04:05:38.067186Z","shell.execute_reply":"2022-11-15T04:10:40.129454Z"},"trusted":true},"execution_count":226,"outputs":[{"name":"stderr","text":"  0%|          | 0/1200 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"100%|█████████▉| 1198/1200 [00:55<00:00, 23.27it/s]","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1200/1200 [00:56<00:00, 21.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nEpochs: 1 | Loss:  0.272 | Accuracy:  0.949 | Val_Loss:  0.214 | Accuracy:  0.957\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/1200 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"100%|█████████▉| 1198/1200 [00:56<00:00, 22.03it/s]","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1200/1200 [00:56<00:00, 21.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nEpochs: 2 | Loss:  0.216 | Accuracy:  0.952 | Val_Loss:  0.196 | Accuracy:  0.958\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/1200 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1200/1200 [00:56<00:00, 22.43it/s]","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1200/1200 [00:56<00:00, 21.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nEpochs: 3 | Loss:  0.185 | Accuracy:  0.955 | Val_Loss:  0.179 | Accuracy:  0.958\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/1200 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"100%|█████████▉| 1199/1200 [00:56<00:00, 23.25it/s]","output_type":"stream"},{"name":"stdout","text":"To disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1200/1200 [00:56<00:00, 21.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nEpochs: 4 | Loss:  0.160 | Accuracy:  0.958 | Val_Loss:  0.183 | Accuracy:  0.958\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/1200 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1200/1200 [00:56<00:00, 21.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nEpochs: 5 | Loss:  0.142 | Accuracy:  0.961 | Val_Loss:  0.185 | Accuracy:  0.956\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### improve the performance of our model. \n\nIf you notice, we have a data imbalance problem as there are a lot of tokens with ‘O’ label. We can improve our model, for example, by applying <b>class weights </b> during the training process.\n\nAlso, you can try different optimizers such as the <b> Adam optimizer with weight decay regularization </b>.","metadata":{}},{"cell_type":"markdown","source":"### Evaluate Model on Test Data\n\nNow that we have trained our model, we can evaluate its performance on unseen test data with the following code snippet.","metadata":{}},{"cell_type":"code","source":"def evaluate(model, df_test):\n\n    test_dataset = DataSequence(df_test)\n\n    test_dataloader = DataLoader(test_dataset, num_workers=4, batch_size=1)\n\n    use_cuda = torch.cuda.is_available()\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n    if use_cuda:\n        model = model.cuda()\n\n    total_acc_test = 0.0\n\n    for test_data, test_label in test_dataloader:\n\n            test_label = test_label.to(device)\n            mask = test_data['attention_mask'].squeeze(1).to(device)\n\n            input_id = test_data['input_ids'].squeeze(1).to(device)\n\n            loss, logits = model(input_id, mask, test_label)\n\n            for i in range(logits.shape[0]):\n\n              logits_clean = logits[i][test_label[i] != -100]\n              label_clean = test_label[i][test_label[i] != -100]\n\n              predictions = logits_clean.argmax(dim=1)\n              acc = (predictions == label_clean).float().mean()\n              total_acc_test += acc\n\n    val_accuracy = total_acc_test / len(df_test)\n    print(f'Test Accuracy: {total_acc_test / len(df_test): .3f}')\n","metadata":{"execution":{"iopub.status.busy":"2022-11-15T04:10:40.133109Z","iopub.execute_input":"2022-11-15T04:10:40.133968Z","iopub.status.idle":"2022-11-15T04:10:40.145223Z","shell.execute_reply.started":"2022-11-15T04:10:40.133921Z","shell.execute_reply":"2022-11-15T04:10:40.143563Z"},"trusted":true},"execution_count":227,"outputs":[]},{"cell_type":"code","source":"evaluate(model, df_test)","metadata":{"execution":{"iopub.status.busy":"2022-11-15T04:10:40.147152Z","iopub.execute_input":"2022-11-15T04:10:40.147593Z","iopub.status.idle":"2022-11-15T04:10:46.842694Z","shell.execute_reply.started":"2022-11-15T04:10:40.147553Z","shell.execute_reply":"2022-11-15T04:10:46.841504Z"},"trusted":true},"execution_count":228,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nTest Accuracy:  0.955\n","output_type":"stream"}]},{"cell_type":"markdown","source":"You can of course, change the metrics to F1 score, precision, or recall.\n\nAlternatively, we can use the trained model to predict the entity of each word of a text or a sentence with the following code:","metadata":{}},{"cell_type":"code","source":"def align_word_ids(texts):\n  \n    tokenized_inputs = tokenizer(texts, padding='max_length', max_length=50, truncation=True)\n\n    word_ids = tokenized_inputs.word_ids()\n\n    previous_word_idx = None\n    label_ids = []\n\n    for word_idx in word_ids:\n\n        if word_idx is None:\n            label_ids.append(-100)\n\n        elif word_idx != previous_word_idx:\n            try:\n                label_ids.append(1)\n            except:\n                label_ids.append(-100)\n        else:\n            try:\n                label_ids.append(1 if label_all_tokens else -100)\n            except:\n                label_ids.append(-100)\n        previous_word_idx = word_idx\n\n    return label_ids\n\n\ndef evaluate_one_text(model, sentence):\n\n\n    use_cuda = torch.cuda.is_available()\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n    if use_cuda:\n        model = model.cuda()\n\n    text = tokenizer(sentence, padding='max_length', max_length = 50, truncation=True, return_tensors=\"pt\")\n\n    mask = text['attention_mask'].to(device)\n    input_id = text['input_ids'].to(device)\n    label_ids = torch.Tensor(align_word_ids(sentence)).unsqueeze(0).to(device)\n\n    logits = model(input_id, mask, None)\n    logits_clean = logits[0][label_ids != -100]\n\n    predictions = logits_clean.argmax(dim=1).tolist()\n    prediction_label = [ids_to_labels[i] for i in predictions]\n    print(sentence)\n    print(prediction_label)","metadata":{"execution":{"iopub.status.busy":"2022-11-15T04:44:19.622843Z","iopub.execute_input":"2022-11-15T04:44:19.623231Z","iopub.status.idle":"2022-11-15T04:44:19.635220Z","shell.execute_reply.started":"2022-11-15T04:44:19.623198Z","shell.execute_reply":"2022-11-15T04:44:19.634106Z"},"trusted":true},"execution_count":234,"outputs":[]},{"cell_type":"code","source":"evaluate_one_text(model, 'Bill Gates is the founder of Microsoft')\n\n#>>> 'Bill Gates is the founder of Microsoft'\n#>>> ['B-per', 'I-per', 'O', 'O', 'O', 'O', 'B-org']","metadata":{"execution":{"iopub.status.busy":"2022-11-15T04:44:26.530841Z","iopub.execute_input":"2022-11-15T04:44:26.531248Z","iopub.status.idle":"2022-11-15T04:44:26.562043Z","shell.execute_reply.started":"2022-11-15T04:44:26.531214Z","shell.execute_reply":"2022-11-15T04:44:26.560885Z"},"trusted":true},"execution_count":235,"outputs":[{"name":"stdout","text":"Bill Gates is the founder of Microsoft\n['B-person', 'I-person', 'O', 'O', 'O', 'O', 'B-corporation']\n","output_type":"stream"}]},{"cell_type":"code","source":"evaluate_one_text(model, 'Elan Musk have recently acquired Twitter')","metadata":{"execution":{"iopub.status.busy":"2022-11-15T04:44:31.390244Z","iopub.execute_input":"2022-11-15T04:44:31.390617Z","iopub.status.idle":"2022-11-15T04:44:31.422617Z","shell.execute_reply.started":"2022-11-15T04:44:31.390586Z","shell.execute_reply":"2022-11-15T04:44:31.421226Z"},"trusted":true},"execution_count":236,"outputs":[{"name":"stdout","text":"Elan Musk have recently acquired Twitter\n['B-person', 'I-person', 'O', 'O', 'O', 'B-corporation']\n","output_type":"stream"}]}]}